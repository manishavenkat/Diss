{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afb6d356-ceed-4ee5-9c3d-d81c72600242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as DatasetTorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, Dataset, Audio\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import librosa\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import colorsys\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix\n",
    "from IPython.display import Audio as ipyaudio \n",
    "import plotly.graph_objs as go\n",
    "from ipywidgets import Output\n",
    "from ipywidgets import VBox\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e987c53-d620-439b-906c-30c3bd02e1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting category names\n",
      "loaded dataset\n"
     ]
    }
   ],
   "source": [
    "train_dir = '/work/tc062/tc062/manishav/huggingface_cache/datasets/speechcolab___gigaspeech/xs/0.0.0/0db31224ad43470c71b459deb2f2b40956b3a4edfde5fb313aaec69ec7b50d3c/gigaspeech-train.arrow'\n",
    "\n",
    "# Load the full datasets\n",
    "full_dataset = Dataset.from_file(train_dir)\n",
    "full_dataset = full_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "print('getting category names')\n",
    "\n",
    "category_names = full_dataset.features['category'].names\n",
    "# Create a dictionary mapping indices to category names\n",
    "category_dict = {i: name for i, name in enumerate(category_names)}\n",
    "\n",
    "print('loaded dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ba074f-f2ea-4259-8036-ce9f2b8bf2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529e2d39dd59477bace10dc29e7c6840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7511 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split data\n"
     ]
    }
   ],
   "source": [
    "def prepare_datasets(dataset):\n",
    "    # Get the total number of samples\n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    # Calculate the number of samples for training (80% of total)\n",
    "    train_samples = int(0.8 * total_samples)\n",
    "    \n",
    "    # Create a random permutation of indices\n",
    "    shuffled_indices = np.random.permutation(total_samples)\n",
    "    \n",
    "    # Split the shuffled indices\n",
    "    train_indices = shuffled_indices[:train_samples]\n",
    "    test_indices = shuffled_indices[train_samples:]\n",
    "    \n",
    "    # Use the select method to create train and test datasets\n",
    "    train_dataset = dataset.select(train_indices)\n",
    "    test_dataset = dataset.select(test_indices)\n",
    "    \n",
    "    # Remove audiobooks from the training set\n",
    "    train_dataset = train_dataset.filter(lambda x: x['category'] != category_names.index('audiobook'))\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset, test_dataset = prepare_datasets(full_dataset)\n",
    "\n",
    "print('split data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ddac02-7eb7-4ddf-9d27-9c78cc9ef547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioUtil():\n",
    "    @staticmethod\n",
    "    def open(audio_file):\n",
    "        sig, sr = torchaudio.load(str(audio_file))\n",
    "        return (sig, sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def rechannel(aud, new_channel):\n",
    "        sig, sr = aud\n",
    "        if sig.shape[0] == new_channel:\n",
    "            return aud\n",
    "        if new_channel == 1:\n",
    "            sig = sig.mean(dim=0, keepdim=True)\n",
    "        else:\n",
    "            sig = sig.expand(new_channel, -1)\n",
    "        return (sig, sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def resample(aud, new_sr):\n",
    "        sig, sr = aud\n",
    "        if sr == new_sr:\n",
    "            return aud\n",
    "        num_channels = sig.shape[0]\n",
    "        resig = torchaudio.transforms.Resample(sr, new_sr)(sig[:1, :])\n",
    "        if num_channels > 1:\n",
    "            retwo = torchaudio.transforms.Resample(sr, new_sr)(sig[1:, :])\n",
    "            resig = torch.cat([resig, retwo])\n",
    "        return (resig, new_sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_trunc(aud, max_ms):\n",
    "        sig, sr = aud\n",
    "        num_rows, sig_len = sig.shape\n",
    "        max_len = sr // 1000 * max_ms\n",
    "        if sig_len > max_len:\n",
    "            sig = sig[:, :max_len]\n",
    "        elif sig_len < max_len:\n",
    "            pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "            pad_end_len = max_len - sig_len - pad_begin_len\n",
    "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "            pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "            sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "        return (sig, sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        sig, sr = aud\n",
    "        top_db = 80\n",
    "        spec = torchaudio.transforms.MelSpectrogram(\n",
    "            sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "        spec = torchaudio.transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        return spec.squeeze(0)  # Remove the channel dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f07eefdf-6b22-481a-9adf-5cccb1d97780",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenreDataset(DatasetTorch):\n",
    "    def __init__(self, dataset, duration=5000, sr=16000, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.duration = duration\n",
    "        self.sr = sr\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        audio_data = item['audio']\n",
    "\n",
    "        # Get the file path\n",
    "        file_path = audio_data.get('path', '')\n",
    "        \n",
    "        # Assuming audio_data is a list with a single dictionary\n",
    "        if isinstance(audio_data, list) and len(audio_data) > 0:\n",
    "            audio_data = audio_data[0]\n",
    "        \n",
    "        # Now audio_data should be a dictionary\n",
    "        if 'array' in audio_data:\n",
    "            if isinstance(audio_data['array'], np.ndarray):\n",
    "                sig = torch.from_numpy(audio_data['array']).float()\n",
    "            else:\n",
    "                # If it's not a numpy array, it might be a list, so convert it\n",
    "                sig = torch.tensor(audio_data['array']).float()\n",
    "        else:\n",
    "            # If 'array' is not present, try to load from 'path'\n",
    "            audio_path = audio_data.get('path')\n",
    "            if audio_path:\n",
    "                sig, sr = torchaudio.load(audio_path)\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot load audio data for item {idx}\")\n",
    "\n",
    "        sr = audio_data.get('sampling_rate', self.sr)\n",
    "\n",
    "        # Ensure the signal is 2D (add channel dimension if necessary)\n",
    "        if sig.dim() == 1:\n",
    "            sig = sig.unsqueeze(0)\n",
    "\n",
    "        # Resample audio to ensure uniform sampling rate\n",
    "        if sr != self.sr:\n",
    "            sig = torchaudio.transforms.Resample(sr, self.sr)(sig)\n",
    "\n",
    "        label = item['category']\n",
    "\n",
    "        aud = (sig, self.sr)  # Ensure uniform sampling rate\n",
    "        aud = AudioUtil.rechannel(aud, 1)\n",
    "        aud = AudioUtil.pad_trunc(aud, self.duration)\n",
    "        sgram = AudioUtil.spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "        \n",
    "        # Remove the channel dimension if it exists\n",
    "        if sgram.dim() == 3:\n",
    "            sgram = sgram.squeeze(0)\n",
    "\n",
    "        if self.transform:\n",
    "            sgram = self.transform(sgram)\n",
    "\n",
    "        return sgram, torch.tensor(label, dtype=torch.long), file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90e2e8bc-e13d-43ae-8993-0015f3a6ad12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting dataloader\n",
      "ending dataloader\n"
     ]
    }
   ],
   "source": [
    "# Create dataset objects\n",
    "train_dataset = GenreDataset(train_dataset)\n",
    "test_dataset = GenreDataset(test_dataset)\n",
    "\n",
    "print('starting dataloader')\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
    "print('ending dataloader')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbabc98a-77ce-411d-94a4-86ad6c078af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=29):\n",
    "        super(AudioClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = None  # We'll define this in the forward pass\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"Input shape to model:\", x.shape)\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        # print(\"Shape after potential unsqueeze:\", x.shape)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # print(\"Shape after first conv and pool:\", x.shape)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        \n",
    "        # Flatten the output\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(\"Shape after flattening:\", x.shape)\n",
    "    \n",
    "        # Dynamically create fc1 if it doesn't exist\n",
    "        if self.fc1 is None:\n",
    "            self.fc1 = nn.Linear(x.shape[1], 128).to(x.device)\n",
    "        \n",
    "        embeddings = F.relu(self.fc1(x))\n",
    "        x = self.dropout(embeddings)\n",
    "        x = self.fc2(x)\n",
    "        return x, embeddings\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AudioClassifier().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "520093e0-a519-4b69-8bef-7be381ba71b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 177/177 [00:13<00:00, 13.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0, Train Loss: 1.8707\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=1):\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in tqdm(train_loader):\n",
    "            inputs, labels, _ = batch\n",
    "            inputs = torch.stack(inputs)\n",
    "            labels = torch.stack(labels)\n",
    "            if inputs.dim() == 3:\n",
    "                inputs = inputs.unsqueeze(1)\n",
    "            \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}, Train Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    # Save the loss plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(\"gs-embeddings\", f\"loss_plot_{num_epochs}epochs_2Kepochs_no-audiobook-training.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5037448a-517d-4788-bb64-644d0e0d3bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on test dataset\n",
    "model.eval()\n",
    "test_embeddings = []\n",
    "test_labels = []\n",
    "test_predictions = []\n",
    "test_file_paths = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels, paths = batch\n",
    "        inputs = torch.stack(inputs)\n",
    "        labels = torch.stack(labels)\n",
    "        if inputs.dim() == 3:\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "        inputs = inputs.to(device)\n",
    "        outputs, emb = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        test_embeddings.append(emb.cpu().numpy())\n",
    "        test_labels.append(labels.cpu().numpy())\n",
    "        test_predictions.append(preds.cpu().numpy())\n",
    "        test_file_paths.extend(paths)\n",
    "\n",
    "test_embeddings = np.concatenate(test_embeddings)\n",
    "test_labels = np.concatenate(test_labels)\n",
    "test_predictions = np.concatenate(test_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bae8d63d-fc37-4d15-a07f-46215a89abc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/tc062/tc062/manishav/.venv/diss/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.3823\n",
      "Test Set F1 Score: 0.2992\n",
      "Test Set Precision: 0.2617\n",
      "Confusion matrix saved as 'confusion_matrix_test_2Kepochs_no-audiobook-training_soundtest.png'\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metrics\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
    "precision = precision_score(test_labels, test_predictions, average='weighted')\n",
    "\n",
    "print(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Set F1 Score: {f1:.4f}\")\n",
    "print(f\"Test Set Precision: {precision:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "all_categories = list(category_dict.values())\n",
    "\n",
    "plt.figure(figsize=(25, 20))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=all_categories,\n",
    "            yticklabels=all_categories)\n",
    "plt.xlabel('Predicted', fontsize=14)\n",
    "plt.ylabel('True', fontsize=14)\n",
    "plt.title('Confusion Matrix for Test Set', fontsize=16)\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_test_2Kepochs_no-audiobook-training_soundtest.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"Confusion matrix saved as 'confusion_matrix_test_2Kepochs_no-audiobook-training_soundtest.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c851df32-79b8-4ce1-9a9d-df4f65433e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio data...\n"
     ]
    }
   ],
   "source": [
    "# T-SNE visualization\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_results = tsne.fit_transform(test_embeddings)\n",
    "\n",
    "def generate_distinct_colors(n):\n",
    "    HSV_tuples = [(x * 1.0 / n, 0.5, 0.5) for x in range(n)]\n",
    "    RGB_tuples = map(lambda x: colorsys.hsv_to_rgb(*x), HSV_tuples)\n",
    "    return ['rgb'+str(tuple(int(255*x) for x in rgb)) for rgb in RGB_tuples]\n",
    "\n",
    "colors = generate_distinct_colors(len(category_dict))\n",
    "color_map = {name: color for name, color in zip(category_dict.values(), colors)}\n",
    "\n",
    "def load_audio(file_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, duration=5)  # Load first 5 seconds\n",
    "        return y, sr\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'x': tsne_results[:, 0],\n",
    "    'y': tsne_results[:, 1],\n",
    "    'label': test_labels,\n",
    "    'predicted': test_predictions,\n",
    "    'category': [category_dict[l] for l in test_labels],\n",
    "    'predicted_category': [category_dict[p] for p in test_predictions],\n",
    "    'file_path': test_file_paths\n",
    "})\n",
    "\n",
    "# Load audio data\n",
    "print(\"Loading audio data...\")\n",
    "df['audio_data'] = df['file_path'].apply(load_audio)\n",
    "\n",
    "def create_audio_widget(audio_data):\n",
    "    y, sr = audio_data\n",
    "    if y is not None and sr is not None:\n",
    "        return ipyaudio(data=y, rate=sr)\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712bc722-ab82-4db2-a493-cb214dde678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Create the plot\n",
    "fig = go.Figure()\n",
    "\n",
    "print('1')\n",
    "\n",
    "for category in df['category'].unique():\n",
    "    category_df = df[df['category'] == category]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=category_df['x'],\n",
    "        y=category_df['y'],\n",
    "        mode='markers',\n",
    "        name=category,\n",
    "        marker=dict(color=color_map[category]),\n",
    "        customdata=category_df[['category', 'predicted_category', 'file_path', 'audio_data']].values,\n",
    "        hovertemplate=(\n",
    "            \"<b>Category:</b> %{customdata[0]}<br>\"\n",
    "            \"<b>Predicted:</b> %{customdata[1]}<br>\"\n",
    "            \"<b>File:</b> %{customdata[2]}<br>\"\n",
    "            \"<extra></extra>\"\n",
    "        )\n",
    "    ))\n",
    "\n",
    "print('2')\n",
    "\n",
    "fig.update_layout(\n",
    "    title='T-SNE of Test Set Embeddings',\n",
    "    legend_title_text='Categories',\n",
    "    legend=dict(\n",
    "        itemsizing='constant',\n",
    "        title_font_family='Arial',\n",
    "        font=dict(family='Arial', size=10),\n",
    "        itemwidth=30\n",
    "    )\n",
    ")\n",
    "\n",
    "print('3')\n",
    "\n",
    "# Create an Output widget for the audio\n",
    "output = widgets.Output()\n",
    "\n",
    "@output.capture()\n",
    "def on_click(trace, points, state):\n",
    "    if points.point_inds:\n",
    "        ind = points.point_inds[0]\n",
    "        audio_data = trace.customdata[ind][3]\n",
    "        audio_widget = create_audio_widget(audio_data)\n",
    "        if audio_widget:\n",
    "            display(audio_widget)\n",
    "\n",
    "# Attach the click event to all traces\n",
    "for trace in fig.data:\n",
    "    trace.on_click(on_click)\n",
    "\n",
    "print('4')\n",
    "# Display the plot\n",
    "display(fig)\n",
    "\n",
    "# Display the output widget for audio\n",
    "display(output)\n",
    "print('5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86659e9-a6e5-42a6-a023-2efabf63c8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
